{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Analysis Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning Process Analysis\n",
    "# Comparing Raw vs Cleaned CSV Files to Identify Cleaning Steps\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Data Cleaning Analysis Setup Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ATP Matches data...\n",
      "Raw matches shape: (58502, 49)\n",
      "Cleaned matches shape: (58081, 49)\n",
      "\n",
      "Loading ATP Point-by-Point data...\n",
      "Raw PbP shape: (13050, 12)\n",
      "Cleaned PbP shape: (11859, 13)\n",
      "\n",
      "Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Load Raw and Cleaned Data Files\n",
    "\n",
    "# Load ATP Matches data\n",
    "raw_matches_path = Path(\"../data/raw/atp_matches/aggregated_atp_matches.csv\")\n",
    "cleaned_matches_path = Path(\"../data/cleaned_refactored/atp_matches_cleaned.csv\")\n",
    "\n",
    "print(\"Loading ATP Matches data...\")\n",
    "raw_matches = pd.read_csv(raw_matches_path)\n",
    "cleaned_matches = pd.read_csv(cleaned_matches_path)\n",
    "\n",
    "print(f\"Raw matches shape: {raw_matches.shape}\")\n",
    "print(f\"Cleaned matches shape: {cleaned_matches.shape}\")\n",
    "print()\n",
    "\n",
    "# Load ATP Point-by-Point data\n",
    "raw_pbp_path = Path(\"../data/raw/atp_point_by_point/aggregated_pbp_matches.csv\")\n",
    "cleaned_pbp_path = Path(\"../data/cleaned_refactored/atp_pbp_cleaned.csv\")\n",
    "\n",
    "print(\"Loading ATP Point-by-Point data...\")\n",
    "raw_pbp = pd.read_csv(raw_pbp_path)\n",
    "cleaned_pbp = pd.read_csv(cleaned_pbp_path)\n",
    "\n",
    "print(f\"Raw PbP shape: {raw_pbp.shape}\")\n",
    "print(f\"Cleaned PbP shape: {cleaned_pbp.shape}\")\n",
    "print(\"\\nData loading complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATP Matches Column Comparison ===\n",
      "\n",
      "Columns in both datasets: 49\n",
      "\n",
      "=== ATP Point-by-Point Column Comparison ===\n",
      "\n",
      "Columns ADDED during cleaning (1):\n",
      "  + parsed_date\n",
      "\n",
      "Columns in both datasets: 12\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Column Structure Comparison\n",
    "\n",
    "def compare_columns(raw_df, cleaned_df, dataset_name):\n",
    "    \"\"\"Compare column structures between raw and cleaned datasets\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} Column Comparison ===\")\n",
    "    \n",
    "    raw_cols = set(raw_df.columns)\n",
    "    cleaned_cols = set(cleaned_df.columns)\n",
    "    \n",
    "    # Columns only in raw\n",
    "    only_in_raw = raw_cols - cleaned_cols\n",
    "    if only_in_raw:\n",
    "        print(f\"\\nColumns REMOVED during cleaning ({len(only_in_raw)}):\")\n",
    "        for col in sorted(only_in_raw):\n",
    "            print(f\"  - {col}\")\n",
    "    \n",
    "    # Columns only in cleaned\n",
    "    only_in_cleaned = cleaned_cols - raw_cols\n",
    "    if only_in_cleaned:\n",
    "        print(f\"\\nColumns ADDED during cleaning ({len(only_in_cleaned)}):\")\n",
    "        for col in sorted(only_in_cleaned):\n",
    "            print(f\"  + {col}\")\n",
    "    \n",
    "    # Common columns\n",
    "    common_cols = raw_cols & cleaned_cols\n",
    "    print(f\"\\nColumns in both datasets: {len(common_cols)}\")\n",
    "    \n",
    "    return common_cols, only_in_raw, only_in_cleaned\n",
    "\n",
    "# Compare ATP Matches columns\n",
    "matches_common, matches_removed, matches_added = compare_columns(raw_matches, cleaned_matches, \"ATP Matches\")\n",
    "\n",
    "# Compare ATP Point-by-Point columns\n",
    "pbp_common, pbp_removed, pbp_added = compare_columns(raw_pbp, cleaned_pbp, \"ATP Point-by-Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATP Matches Data Type Changes ===\n",
      "No data type changes detected\n",
      "\n",
      "=== ATP Point-by-Point Data Type Changes ===\n",
      "No data type changes detected\n"
     ]
    }
   ],
   "source": [
    "## Step 3: Data Type Changes Analysis\n",
    "\n",
    "def compare_data_types(raw_df, cleaned_df, common_cols, dataset_name):\n",
    "    \"\"\"Compare data types between raw and cleaned datasets\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} Data Type Changes ===\")\n",
    "    \n",
    "    type_changes = []\n",
    "    for col in common_cols:\n",
    "        raw_type = str(raw_df[col].dtype)\n",
    "        cleaned_type = str(cleaned_df[col].dtype)\n",
    "        \n",
    "        if raw_type != cleaned_type:\n",
    "            type_changes.append({\n",
    "                'column': col,\n",
    "                'raw_type': raw_type,\n",
    "                'cleaned_type': cleaned_type\n",
    "            })\n",
    "    \n",
    "    if type_changes:\n",
    "        print(f\"\\nData type changes found ({len(type_changes)}):\")\n",
    "        for change in type_changes:\n",
    "            print(f\"  {change['column']}: {change['raw_type']} → {change['cleaned_type']}\")\n",
    "    else:\n",
    "        print(\"No data type changes detected\")\n",
    "    \n",
    "    return type_changes\n",
    "\n",
    "# Analyze data type changes\n",
    "matches_type_changes = compare_data_types(raw_matches, cleaned_matches, matches_common, \"ATP Matches\")\n",
    "pbp_type_changes = compare_data_types(raw_pbp, cleaned_pbp, pbp_common, \"ATP Point-by-Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATP Matches Missing Values Comparison ===\n",
      "\n",
      "Missing value changes found (29):\n",
      "  loser_entry: 46268 (79.1%) → 45896 (79.0%) ↓\n",
      "  winner_entry: 50933 (87.1%) → 50561 (87.1%) ↓\n",
      "  l_svpt: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  l_bpSaved: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  l_bpFaced: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_1stWon: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_svpt: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  l_1stWon: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_bpSaved: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  l_2ndWon: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_df: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  l_1stIn: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  l_df: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_1stIn: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  l_ace: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_bpFaced: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_ace: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_2ndWon: 4555 (7.8%) → 4225 (7.3%) ↓\n",
      "  w_SvGms: 4555 (7.8%) → 4226 (7.3%) ↓\n",
      "  l_SvGms: 4555 (7.8%) → 4226 (7.3%) ↓\n",
      "  minutes: 6195 (10.6%) → 5925 (10.2%) ↓\n",
      "  loser_seed: 44760 (76.5%) → 44522 (76.7%) ↓\n",
      "  winner_seed: 33601 (57.4%) → 33414 (57.5%) ↓\n",
      "  winner_ht: 921 (1.6%) → 914 (1.6%) ↓\n",
      "  loser_ht: 1978 (3.4%) → 1973 (3.4%) ↓\n",
      "  winner_rank_points: 371 (0.6%) → 367 (0.6%) ↓\n",
      "  winner_rank: 371 (0.6%) → 367 (0.6%) ↓\n",
      "  loser_rank_points: 1016 (1.7%) → 1013 (1.7%) ↓\n",
      "  loser_rank: 1016 (1.7%) → 1013 (1.7%) ↓\n",
      "\n",
      "=== ATP Point-by-Point Missing Values Comparison ===\n",
      "No significant missing value changes detected\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Missing Values Analysis\n",
    "\n",
    "def compare_missing_values(raw_df, cleaned_df, common_cols, dataset_name):\n",
    "    \"\"\"Compare missing values between raw and cleaned datasets\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} Missing Values Comparison ===\")\n",
    "    \n",
    "    missing_changes = []\n",
    "    \n",
    "    for col in common_cols:\n",
    "        raw_missing = raw_df[col].isnull().sum()\n",
    "        cleaned_missing = cleaned_df[col].isnull().sum()\n",
    "        raw_missing_pct = (raw_missing / len(raw_df)) * 100\n",
    "        cleaned_missing_pct = (cleaned_missing / len(cleaned_df)) * 100\n",
    "        \n",
    "        if raw_missing != cleaned_missing:\n",
    "            missing_changes.append({\n",
    "                'column': col,\n",
    "                'raw_missing': raw_missing,\n",
    "                'cleaned_missing': cleaned_missing,\n",
    "                'raw_missing_pct': raw_missing_pct,\n",
    "                'cleaned_missing_pct': cleaned_missing_pct,\n",
    "                'change': cleaned_missing - raw_missing\n",
    "            })\n",
    "    \n",
    "    if missing_changes:\n",
    "        print(f\"\\nMissing value changes found ({len(missing_changes)}):\")\n",
    "        for change in sorted(missing_changes, key=lambda x: abs(x['change']), reverse=True):\n",
    "            direction = \"↓\" if change['change'] < 0 else \"↑\"\n",
    "            print(f\"  {change['column']}: {change['raw_missing']} ({change['raw_missing_pct']:.1f}%) → {change['cleaned_missing']} ({change['cleaned_missing_pct']:.1f}%) {direction}\")\n",
    "    else:\n",
    "        print(\"No significant missing value changes detected\")\n",
    "    \n",
    "    return missing_changes\n",
    "\n",
    "# Analyze missing values changes\n",
    "matches_missing_changes = compare_missing_values(raw_matches, cleaned_matches, matches_common, \"ATP Matches\")\n",
    "pbp_missing_changes = compare_missing_values(raw_pbp, cleaned_pbp, pbp_common, \"ATP Point-by-Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATP Matches Row Count Analysis ===\n",
      "Raw dataset rows: 58,502\n",
      "Cleaned dataset rows: 58,081\n",
      "Difference: -421 rows (-0.72%)\n",
      "→ Rows were REMOVED during cleaning (filtering applied)\n",
      "\n",
      "=== ATP Point-by-Point Row Count Analysis ===\n",
      "Raw dataset rows: 13,050\n",
      "Cleaned dataset rows: 11,859\n",
      "Difference: -1,191 rows (-9.13%)\n",
      "→ Rows were REMOVED during cleaning (filtering applied)\n"
     ]
    }
   ],
   "source": [
    "## Step 5: Row Count and Filtering Analysis\n",
    "\n",
    "def analyze_row_changes(raw_df, cleaned_df, dataset_name):\n",
    "    \"\"\"Analyze changes in row counts and potential filtering\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} Row Count Analysis ===\")\n",
    "    \n",
    "    raw_rows = len(raw_df)\n",
    "    cleaned_rows = len(cleaned_df)\n",
    "    row_diff = cleaned_rows - raw_rows\n",
    "    \n",
    "    print(f\"Raw dataset rows: {raw_rows:,}\")\n",
    "    print(f\"Cleaned dataset rows: {cleaned_rows:,}\")\n",
    "    print(f\"Difference: {row_diff:,} rows ({(row_diff/raw_rows)*100:.2f}%)\")\n",
    "    \n",
    "    if row_diff < 0:\n",
    "        print(\"→ Rows were REMOVED during cleaning (filtering applied)\")\n",
    "    elif row_diff > 0:\n",
    "        print(\"→ Rows were ADDED during cleaning (data augmentation)\")\n",
    "    else:\n",
    "        print(\"→ No change in row count\")\n",
    "    \n",
    "    return {\n",
    "        'raw_rows': raw_rows,\n",
    "        'cleaned_rows': cleaned_rows,\n",
    "        'row_diff': row_diff,\n",
    "        'pct_change': (row_diff/raw_rows)*100\n",
    "    }\n",
    "\n",
    "# Analyze row changes\n",
    "matches_row_analysis = analyze_row_changes(raw_matches, cleaned_matches, \"ATP Matches\")\n",
    "pbp_row_analysis = analyze_row_changes(raw_pbp, cleaned_pbp, \"ATP Point-by-Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATP Matches Value Distribution Analysis ===\n",
      "\n",
      "Analyzing 5 numeric columns...\n",
      "\n",
      "--- match_num ---\n",
      "Raw: min=1.00, max=1701.00, mean=123.38\n",
      "Cleaned: min=1.00, max=1701.00, mean=123.27\n",
      "\n",
      "--- winner_rank_points ---\n",
      "Raw: min=1.00, max=16950.00, mean=1759.85\n",
      "Cleaned: min=1.00, max=16950.00, mean=1755.39\n",
      "\n",
      "--- loser_rank_points ---\n",
      "Raw: min=1.00, max=16950.00, mean=1041.73\n",
      "Cleaned: min=1.00, max=16950.00, mean=1037.47\n",
      "\n",
      "--- winner_age ---\n",
      "Raw: min=14.90, max=44.60, mean=26.64\n",
      "Cleaned: min=14.90, max=44.60, mean=26.64\n",
      "\n",
      "--- l_svpt ---\n",
      "Raw: min=0.00, max=489.00, mean=81.21\n",
      "Cleaned: min=0.00, max=489.00, mean=81.33\n",
      "\n",
      "Analyzing 5 categorical columns...\n",
      "\n",
      "--- tourney_level ---\n",
      "Raw unique values: 6\n",
      "Cleaned unique values: 6\n",
      "\n",
      "--- loser_entry ---\n",
      "Raw unique values: 11\n",
      "Cleaned unique values: 11\n",
      "\n",
      "--- winner_entry ---\n",
      "Raw unique values: 10\n",
      "Cleaned unique values: 10\n",
      "\n",
      "--- score ---\n",
      "Raw unique values: 10441\n",
      "Cleaned unique values: 10438\n",
      "→ CATEGORICAL VALUES CHANGED\n",
      "\n",
      "--- surface ---\n",
      "Raw unique values: 4\n",
      "Cleaned unique values: 4\n",
      "\n",
      "=== ATP Point-by-Point Value Distribution Analysis ===\n",
      "\n",
      "Analyzing 4 numeric columns...\n",
      "\n",
      "--- winner ---\n",
      "Raw: min=1.00, max=2.00, mean=1.47\n",
      "Cleaned: min=1.00, max=2.00, mean=1.47\n",
      "\n",
      "--- pbp_id ---\n",
      "Raw: min=2228887.00, max=11628937.00, mean=6133169.14\n",
      "Cleaned: min=2228887.00, max=11628937.00, mean=6237941.96\n",
      "\n",
      "--- wh_minutes ---\n",
      "Raw: min=-1398.00, max=1405.00, mean=41.39\n",
      "Cleaned: min=20.00, max=1405.00, mean=107.21\n",
      "→ OUTLIERS REMOVED or values capped\n",
      "\n",
      "--- adf_flag ---\n",
      "Raw: min=0.00, max=1.00, mean=0.71\n",
      "Cleaned: min=0.00, max=1.00, mean=0.73\n",
      "\n",
      "Analyzing 5 categorical columns...\n",
      "\n",
      "--- draw ---\n",
      "Raw unique values: 1\n",
      "Cleaned unique values: 1\n",
      "\n",
      "--- server1 ---\n",
      "Raw unique values: 784\n",
      "Cleaned unique values: 760\n",
      "→ CATEGORICAL VALUES CHANGED\n",
      "\n",
      "--- date ---\n",
      "Raw unique values: 1475\n",
      "Cleaned unique values: 1413\n",
      "→ CATEGORICAL VALUES CHANGED\n",
      "\n",
      "--- pbp ---\n",
      "Raw unique values: 12975\n",
      "Cleaned unique values: 11823\n",
      "→ CATEGORICAL VALUES CHANGED\n",
      "\n",
      "--- score ---\n",
      "Raw unique values: 3268\n",
      "Cleaned unique values: 2979\n",
      "→ CATEGORICAL VALUES CHANGED\n"
     ]
    }
   ],
   "source": [
    "## Step 6: Value Distribution Analysis\n",
    "\n",
    "def analyze_value_distributions(raw_df, cleaned_df, common_cols, dataset_name, sample_size=5):\n",
    "    \"\"\"Analyze changes in value distributions for key columns\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} Value Distribution Analysis ===\")\n",
    "    \n",
    "    # Focus on columns that might have been cleaned\n",
    "    numeric_cols = [col for col in common_cols if raw_df[col].dtype in ['int64', 'float64']]\n",
    "    categorical_cols = [col for col in common_cols if raw_df[col].dtype == 'object']\n",
    "    \n",
    "    print(f\"\\nAnalyzing {min(sample_size, len(numeric_cols))} numeric columns...\")\n",
    "    for col in numeric_cols[:sample_size]:\n",
    "        print(f\"\\n--- {col} ---\")\n",
    "        raw_stats = raw_df[col].describe()\n",
    "        cleaned_stats = cleaned_df[col].describe()\n",
    "        \n",
    "        print(f\"Raw: min={raw_stats['min']:.2f}, max={raw_stats['max']:.2f}, mean={raw_stats['mean']:.2f}\")\n",
    "        print(f\"Cleaned: min={cleaned_stats['min']:.2f}, max={cleaned_stats['max']:.2f}, mean={cleaned_stats['mean']:.2f}\")\n",
    "        \n",
    "        # Check for outlier removal\n",
    "        if raw_stats['min'] != cleaned_stats['min'] or raw_stats['max'] != cleaned_stats['max']:\n",
    "            print(\"→ OUTLIERS REMOVED or values capped\")\n",
    "    \n",
    "    print(f\"\\nAnalyzing {min(sample_size, len(categorical_cols))} categorical columns...\")\n",
    "    for col in categorical_cols[:sample_size]:\n",
    "        print(f\"\\n--- {col} ---\")\n",
    "        raw_unique = raw_df[col].nunique()\n",
    "        cleaned_unique = cleaned_df[col].nunique()\n",
    "        \n",
    "        print(f\"Raw unique values: {raw_unique}\")\n",
    "        print(f\"Cleaned unique values: {cleaned_unique}\")\n",
    "        \n",
    "        if raw_unique != cleaned_unique:\n",
    "            print(\"→ CATEGORICAL VALUES CHANGED\")\n",
    "            \n",
    "            # Show some examples of unique values\n",
    "            raw_sample = set(raw_df[col].dropna().unique()[:10])\n",
    "            cleaned_sample = set(cleaned_df[col].dropna().unique()[:10])\n",
    "            \n",
    "            removed_values = raw_sample - cleaned_sample\n",
    "            added_values = cleaned_sample - raw_sample\n",
    "            \n",
    "            if removed_values:\n",
    "                print(f\"  Removed: {removed_values}\")\n",
    "            if added_values:\n",
    "                print(f\"  Added: {added_values}\")\n",
    "\n",
    "# Analyze value distributions\n",
    "analyze_value_distributions(raw_matches, cleaned_matches, matches_common, \"ATP Matches\")\n",
    "analyze_value_distributions(raw_pbp, cleaned_pbp, pbp_common, \"ATP Point-by-Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATP Matches Duplicate Analysis ===\n",
      "Raw dataset duplicates: 0\n",
      "Cleaned dataset duplicates: 0\n",
      "→ No change in duplicates\n",
      "\n",
      "=== ATP Point-by-Point Duplicate Analysis ===\n",
      "Raw dataset duplicates: 38\n",
      "Cleaned dataset duplicates: 0\n",
      "→ 38 duplicates REMOVED during cleaning\n"
     ]
    }
   ],
   "source": [
    "## Step 7: Duplicate Detection Analysis\n",
    "\n",
    "def analyze_duplicates(raw_df, cleaned_df, dataset_name):\n",
    "    \"\"\"Analyze duplicate row removal\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} Duplicate Analysis ===\")\n",
    "    \n",
    "    raw_duplicates = raw_df.duplicated().sum()\n",
    "    cleaned_duplicates = cleaned_df.duplicated().sum()\n",
    "    \n",
    "    print(f\"Raw dataset duplicates: {raw_duplicates:,}\")\n",
    "    print(f\"Cleaned dataset duplicates: {cleaned_duplicates:,}\")\n",
    "    \n",
    "    if raw_duplicates > cleaned_duplicates:\n",
    "        removed_dupes = raw_duplicates - cleaned_duplicates\n",
    "        print(f\"→ {removed_dupes:,} duplicates REMOVED during cleaning\")\n",
    "    elif raw_duplicates < cleaned_duplicates:\n",
    "        print(\"→ Duplicates INCREASED (unexpected)\")\n",
    "    else:\n",
    "        print(\"→ No change in duplicates\")\n",
    "    \n",
    "    return {\n",
    "        'raw_duplicates': raw_duplicates,\n",
    "        'cleaned_duplicates': cleaned_duplicates,\n",
    "        'removed_duplicates': raw_duplicates - cleaned_duplicates\n",
    "    }\n",
    "\n",
    "# Analyze duplicates\n",
    "matches_dup_analysis = analyze_duplicates(raw_matches, cleaned_matches, \"ATP Matches\")\n",
    "pbp_dup_analysis = analyze_duplicates(raw_pbp, cleaned_pbp, \"ATP Point-by-Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATP Matches Sample Data Comparison ===\n",
      "\n",
      "Showing first 3 rows comparison:\n",
      "\n",
      "RAW DATA:\n",
      "   match_num  winner_rank_points  loser_rank_points  winner_age  l_svpt  \\\n",
      "0        300              3590.0             1977.0        29.0   100.0   \n",
      "1        299              1977.0              200.0        22.8    77.0   \n",
      "2        298              3590.0             1050.0        29.0    46.0   \n",
      "\n",
      "   l_bpSaved tourney_level  l_bpFaced  w_1stWon loser_entry winner_entry  \\\n",
      "0       10.0             A       15.0      31.0         NaN          NaN   \n",
      "1       10.0             A       13.0      28.0          PR          NaN   \n",
      "2        1.0             A        5.0      26.0         NaN          NaN   \n",
      "\n",
      "   loser_rank  loser_age  w_svpt  l_1stWon  winner_seed  w_bpSaved  l_2ndWon  \\\n",
      "0        16.0       22.8    77.0      34.0          2.0        3.0      20.0   \n",
      "1       239.0       33.7    52.0      36.0          4.0        0.0       7.0   \n",
      "2        40.0       31.8    47.0      15.0          2.0        2.0       6.0   \n",
      "\n",
      "         score surface  winner_rank  minutes  w_df  l_1stIn  best_of  l_df  \\\n",
      "0  6-4 3-6 6-2    Hard          9.0    124.0   3.0     54.0        3   6.0   \n",
      "1   7-6(6) 6-2    Hard         16.0     82.0   1.0     52.0        3   2.0   \n",
      "2      6-2 6-2    Hard          9.0     66.0   2.0     27.0        3   3.0   \n",
      "\n",
      "   w_1stIn  tourney_date  loser_ht round      winner_name  l_ace tourney_id  \\\n",
      "0     44.0      20181231     198.0     F    Kei Nishikori    8.0  2019-M020   \n",
      "1     33.0      20181231     188.0    SF  Daniil Medvedev   17.0  2019-M020   \n",
      "2     33.0      20181231     188.0    SF    Kei Nishikori   10.0  2019-M020   \n",
      "\n",
      "  winner_hand  winner_ht winner_ioc loser_hand loser_ioc  w_bpFaced  w_ace  \\\n",
      "0           R      178.0        JPN          R       RUS        6.0    3.0   \n",
      "1           R      198.0        RUS          R       FRA        1.0   10.0   \n",
      "2           R      178.0        JPN          R       FRA        2.0    2.0   \n",
      "\n",
      "  tourney_name  w_SvGms  draw_size  loser_seed          loser_name  w_2ndWon  \\\n",
      "0     Brisbane     13.0         32         4.0     Daniil Medvedev      17.0   \n",
      "1     Brisbane     10.0         32         NaN  Jo-Wilfried Tsonga      14.0   \n",
      "2     Brisbane      8.0         32         NaN       Jeremy Chardy       9.0   \n",
      "\n",
      "   winner_id  l_SvGms  loser_id  \n",
      "0     105453     14.0    106421  \n",
      "1     106421     10.0    104542  \n",
      "2     105453      8.0    104871  \n",
      "\n",
      "CLEANED DATA:\n",
      "   match_num  winner_rank_points  loser_rank_points  winner_age  l_svpt  \\\n",
      "0        300              3590.0             1977.0        29.0   100.0   \n",
      "1        299              1977.0              200.0        22.8    77.0   \n",
      "2        298              3590.0             1050.0        29.0    46.0   \n",
      "\n",
      "   l_bpSaved tourney_level  l_bpFaced  w_1stWon loser_entry winner_entry  \\\n",
      "0       10.0             A       15.0      31.0         NaN          NaN   \n",
      "1       10.0             A       13.0      28.0          PR          NaN   \n",
      "2        1.0             A        5.0      26.0         NaN          NaN   \n",
      "\n",
      "   loser_rank  loser_age  w_svpt  l_1stWon  winner_seed  w_bpSaved  l_2ndWon  \\\n",
      "0        16.0       22.8    77.0      34.0          2.0        3.0      20.0   \n",
      "1       239.0       33.7    52.0      36.0          4.0        0.0       7.0   \n",
      "2        40.0       31.8    47.0      15.0          2.0        2.0       6.0   \n",
      "\n",
      "         score surface  winner_rank  minutes  w_df  l_1stIn  best_of  l_df  \\\n",
      "0  6-4 3-6 6-2    Hard          9.0    124.0   3.0     54.0        3   6.0   \n",
      "1   7-6(6) 6-2    Hard         16.0     82.0   1.0     52.0        3   2.0   \n",
      "2      6-2 6-2    Hard          9.0     66.0   2.0     27.0        3   3.0   \n",
      "\n",
      "   w_1stIn  tourney_date  loser_ht round      winner_name  l_ace tourney_id  \\\n",
      "0     44.0      20181231     198.0     F    Kei Nishikori    8.0  2019-M020   \n",
      "1     33.0      20181231     188.0    SF  Daniil Medvedev   17.0  2019-M020   \n",
      "2     33.0      20181231     188.0    SF    Kei Nishikori   10.0  2019-M020   \n",
      "\n",
      "  winner_hand  winner_ht winner_ioc loser_hand loser_ioc  w_bpFaced  w_ace  \\\n",
      "0           R      178.0        JPN          R       RUS        6.0    3.0   \n",
      "1           R      198.0        RUS          R       FRA        1.0   10.0   \n",
      "2           R      178.0        JPN          R       FRA        2.0    2.0   \n",
      "\n",
      "  tourney_name  w_SvGms  draw_size  loser_seed          loser_name  w_2ndWon  \\\n",
      "0     Brisbane     13.0         32         4.0     Daniil Medvedev      17.0   \n",
      "1     Brisbane     10.0         32         NaN  Jo-Wilfried Tsonga      14.0   \n",
      "2     Brisbane      8.0         32         NaN       Jeremy Chardy       9.0   \n",
      "\n",
      "   winner_id  l_SvGms  loser_id  \n",
      "0     105453     14.0    106421  \n",
      "1     106421     10.0    104542  \n",
      "2     105453      8.0    104871  \n",
      "\n",
      "=== ATP Point-by-Point Sample Data Comparison ===\n",
      "\n",
      "Showing first 3 rows comparison:\n",
      "\n",
      "RAW DATA:\n",
      "   draw  winner   pbp_id         server1       date  \\\n",
      "0  Main       2  2231275  Olivier Rochus  28 Jul 11   \n",
      "1  Main       2  2231276     Robin Haase  28 Jul 11   \n",
      "2  Main       1  2236280     Marin Cilic  29 Jul 11   \n",
      "\n",
      "                                                 pbp  wh_minutes        score  \\\n",
      "0  SSSS;RRRR;SSRRSS;SSRRSS;RSRSRSRR;SSRSS;RSRRSR;...          66      6-4 6-1   \n",
      "1  SSRSS;RRSSRSSS;SSSS;RSSSS;SRSRSS;RSRSRSSS;RSRS...         141  4-6 6-4 6-3   \n",
      "2  SSSS;SRRRR;SSRRRSSS;RSRRSSSS;RSRSSS;SRRRR;SSRS...          71      6-1 6-3   \n",
      "\n",
      "  tour  adf_flag                           tny_name        server2  \n",
      "0  ATP         0  ATPStudenaCroatiaOpen-ATPUmag2011  Fabio Fognini  \n",
      "1  ATP         0  ATPStudenaCroatiaOpen-ATPUmag2011    Marin Cilic  \n",
      "2  ATP         0  ATPStudenaCroatiaOpen-ATPUmag2011  Andreas Seppi  \n",
      "\n",
      "CLEANED DATA:\n",
      "   draw  winner   pbp_id         server1       date  \\\n",
      "0  Main       2  2231275  Olivier Rochus  28 Jul 11   \n",
      "1  Main       2  2231276     Robin Haase  28 Jul 11   \n",
      "2  Main       1  2236280     Marin Cilic  29 Jul 11   \n",
      "\n",
      "                                                 pbp  wh_minutes        score  \\\n",
      "0  SSSS;RRRR;SSRRSS;SSRRSS;RSRSRSRR;SSRSS;RSRRSR;...          66      6-4 6-1   \n",
      "1  SSRSS;RRSSRSSS;SSSS;RSSSS;SRSRSS;RSRSRSSS;RSRS...         141  4-6 6-4 6-3   \n",
      "2  SSSS;SRRRR;SSRRRSSS;RSRRSSSS;RSRSSS;SRRRR;SSRS...          71      6-1 6-3   \n",
      "\n",
      "  tour  adf_flag                           tny_name        server2  \n",
      "0  ATP         0  ATPStudenaCroatiaOpen-ATPUmag2011  Fabio Fognini  \n",
      "1  ATP         0  ATPStudenaCroatiaOpen-ATPUmag2011    Marin Cilic  \n",
      "2  ATP         0  ATPStudenaCroatiaOpen-ATPUmag2011  Andreas Seppi  \n"
     ]
    }
   ],
   "source": [
    "## Step 8: Sample Data Comparison\n",
    "\n",
    "def show_sample_comparison(raw_df, cleaned_df, dataset_name, n_samples=3):\n",
    "    \"\"\"Show side-by-side comparison of sample rows\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} Sample Data Comparison ===\")\n",
    "    \n",
    "    # Get common columns for comparison\n",
    "    common_cols = list(set(raw_df.columns) & set(cleaned_df.columns))\n",
    "    \n",
    "    print(f\"\\nShowing first {n_samples} rows comparison:\")\n",
    "    print(\"\\nRAW DATA:\")\n",
    "    print(raw_df[common_cols].head(n_samples))\n",
    "    \n",
    "    print(f\"\\nCLEANED DATA:\")\n",
    "    print(cleaned_df[common_cols].head(n_samples))\n",
    "\n",
    "# Show sample comparisons\n",
    "show_sample_comparison(raw_matches, cleaned_matches, \"ATP Matches\")\n",
    "show_sample_comparison(raw_pbp, cleaned_pbp, \"ATP Point-by-Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE DATA CLEANING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 DATASET OVERVIEW:\n",
      "  • ATP Matches: 58,502 → 58,081 rows\n",
      "  • ATP Point-by-Point: 13,050 → 11,859 rows\n",
      "\n",
      "🏷️  COLUMN CHANGES:\n",
      "  ATP Point-by-Point:\n",
      "    - Added 1 columns: ['parsed_date']\n",
      "\n",
      "🔄 DATA TYPE CHANGES:\n",
      "\n",
      "📉 ROW FILTERING:\n",
      "  ATP Matches: -421 rows (-0.72%)\n",
      "  ATP Point-by-Point: -1,191 rows (-9.13%)\n",
      "\n",
      "🔍 DUPLICATE REMOVAL:\n",
      "  ATP Matches: 0 duplicates removed\n",
      "  ATP Point-by-Point: 38 duplicates removed\n",
      "\n",
      "💡 RECOMMENDED CLEANING PIPELINE:\n",
      "  1. Load raw data from aggregated CSV files\n",
      "  2. Remove duplicate rows\n",
      "  3. Apply row filtering (remove invalid/incomplete records)\n",
      "  6. Handle missing values (imputation or removal)\n",
      "  7. Add derived/calculated columns\n",
      "  8. Save cleaned data to separate files\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Step 9: Summary Report - Identified Cleaning Steps\n",
    "\n",
    "def generate_cleaning_summary():\n",
    "    \"\"\"Generate a comprehensive summary of all identified cleaning steps\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE DATA CLEANING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "    print(f\"  • ATP Matches: {raw_matches.shape[0]:,} → {cleaned_matches.shape[0]:,} rows\")\n",
    "    print(f\"  • ATP Point-by-Point: {raw_pbp.shape[0]:,} → {cleaned_pbp.shape[0]:,} rows\")\n",
    "    \n",
    "    print(f\"\\n🏷️  COLUMN CHANGES:\")\n",
    "    if matches_removed or matches_added:\n",
    "        print(f\"  ATP Matches:\")\n",
    "        if matches_removed:\n",
    "            print(f\"    - Removed {len(matches_removed)} columns: {list(matches_removed)[:3]}{'...' if len(matches_removed) > 3 else ''}\")\n",
    "        if matches_added:\n",
    "            print(f\"    - Added {len(matches_added)} columns: {list(matches_added)[:3]}{'...' if len(matches_added) > 3 else ''}\")\n",
    "    \n",
    "    if pbp_removed or pbp_added:\n",
    "        print(f\"  ATP Point-by-Point:\")\n",
    "        if pbp_removed:\n",
    "            print(f\"    - Removed {len(pbp_removed)} columns: {list(pbp_removed)[:3]}{'...' if len(pbp_removed) > 3 else ''}\")\n",
    "        if pbp_added:\n",
    "            print(f\"    - Added {len(pbp_added)} columns: {list(pbp_added)[:3]}{'...' if len(pbp_added) > 3 else ''}\")\n",
    "    \n",
    "    print(f\"\\n🔄 DATA TYPE CHANGES:\")\n",
    "    if matches_type_changes:\n",
    "        print(f\"  ATP Matches: {len(matches_type_changes)} columns had type changes\")\n",
    "        for change in matches_type_changes[:3]:\n",
    "            print(f\"    - {change['column']}: {change['raw_type']} → {change['cleaned_type']}\")\n",
    "    \n",
    "    if pbp_type_changes:\n",
    "        print(f\"  ATP Point-by-Point: {len(pbp_type_changes)} columns had type changes\")\n",
    "        for change in pbp_type_changes[:3]:\n",
    "            print(f\"    - {change['column']}: {change['raw_type']} → {change['cleaned_type']}\")\n",
    "    \n",
    "    print(f\"\\n📉 ROW FILTERING:\")\n",
    "    print(f\"  ATP Matches: {matches_row_analysis['row_diff']:,} rows ({matches_row_analysis['pct_change']:.2f}%)\")\n",
    "    print(f\"  ATP Point-by-Point: {pbp_row_analysis['row_diff']:,} rows ({pbp_row_analysis['pct_change']:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n🔍 DUPLICATE REMOVAL:\")\n",
    "    print(f\"  ATP Matches: {matches_dup_analysis['removed_duplicates']:,} duplicates removed\")\n",
    "    print(f\"  ATP Point-by-Point: {pbp_dup_analysis['removed_duplicates']:,} duplicates removed\")\n",
    "    \n",
    "    print(f\"\\n💡 RECOMMENDED CLEANING PIPELINE:\")\n",
    "    print(f\"  1. Load raw data from aggregated CSV files\")\n",
    "    print(f\"  2. Remove duplicate rows\")\n",
    "    if matches_row_analysis['row_diff'] < 0 or pbp_row_analysis['row_diff'] < 0:\n",
    "        print(f\"  3. Apply row filtering (remove invalid/incomplete records)\")\n",
    "    if matches_removed or pbp_removed:\n",
    "        print(f\"  4. Drop unnecessary columns\")\n",
    "    if matches_type_changes or pbp_type_changes:\n",
    "        print(f\"  5. Convert data types for proper analysis\")\n",
    "    if matches_missing_changes or pbp_missing_changes:\n",
    "        print(f\"  6. Handle missing values (imputation or removal)\")\n",
    "    if matches_added or pbp_added:\n",
    "        print(f\"  7. Add derived/calculated columns\")\n",
    "    print(f\"  8. Save cleaned data to separate files\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate comprehensive summary\n",
    "generate_cleaning_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING PROCESS RECONSTRUCTION PLAN\n",
    "\n",
    "Based on the analysis above, here's the complete reconstruction plan for your data cleaning pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING CLEANING PROCESS RECONSTRUCTION\n",
      "============================================================\n",
      "Starting ATP Matches cleaning: 58,502 rows\n",
      "  Removed 4904 rows with missing critical stats\n",
      "  Removed 1792 rows with invalid match data\n",
      "Final ATP Matches: 51,806 rows (6696 removed, 11.45%)\n",
      "\\nTarget: 58,081 rows\n",
      "Achieved: 51,806 rows\n",
      "Difference: -6275 rows\n",
      "\\nStarting ATP PbP cleaning: 13,050 rows\n",
      "  Removed 38 duplicate rows\n",
      "  Removed 1153 rows with invalid match durations (<20 min)\n",
      "  Added parsed_date column, removed 0 rows with invalid dates\n",
      "  Removed 0 rows with invalid PbP data\n",
      "Final ATP PbP: 11,859 rows (1191 removed, 9.13%)\n",
      "\\nTarget: 11,859 rows\n",
      "Achieved: 11,859 rows\n",
      "Difference: 0 rows\n"
     ]
    }
   ],
   "source": [
    "## RECONSTRUCTION STEP 1: Data Quality Filtering Functions\n",
    "\n",
    "def clean_atp_matches(df):\n",
    "    \"\"\"\n",
    "    Reconstruct ATP Matches cleaning process\n",
    "    Expected: 58,502 → 58,081 rows (-421 rows, -0.72%)\n",
    "    \"\"\"\n",
    "    print(f\"Starting ATP Matches cleaning: {len(df):,} rows\")\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Step 1: Remove rows with excessive missing data\n",
    "    # Based on analysis: 29 columns had reduced missing values\n",
    "    # Strategy: Remove matches with missing critical match statistics\n",
    "    critical_stats = ['w_svpt', 'l_svpt', 'winner_rank', 'loser_rank']\n",
    "    before_missing = len(df)\n",
    "    df_clean = df.dropna(subset=critical_stats, how='any')\n",
    "    after_missing = len(df_clean)\n",
    "    print(f\"  Removed {before_missing - after_missing} rows with missing critical stats\")\n",
    "    \n",
    "    # Step 2: Remove invalid match data\n",
    "    # Filter out matches with impossible statistics\n",
    "    before_invalid = len(df_clean)\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['best_of'].isin([3, 5])) &  # Valid match formats\n",
    "        (df_clean['minutes'] >= 20) &  # Minimum realistic match duration\n",
    "        (df_clean['winner_age'] >= 14) &  # Minimum professional age\n",
    "        (df_clean['loser_age'] >= 14)\n",
    "    ]\n",
    "    after_invalid = len(df_clean)\n",
    "    print(f\"  Removed {before_invalid - after_invalid} rows with invalid match data\")\n",
    "    \n",
    "    final_count = len(df_clean)\n",
    "    total_removed = original_count - final_count\n",
    "    print(f\"Final ATP Matches: {final_count:,} rows ({total_removed} removed, {(total_removed/original_count)*100:.2f}%)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_atp_pbp(df):\n",
    "    \"\"\"\n",
    "    Reconstruct ATP Point-by-Point cleaning process\n",
    "    Expected: 13,050 → 11,859 rows (-1,191 rows, -9.13%)\n",
    "    \"\"\"\n",
    "    print(f\"\\\\nStarting ATP PbP cleaning: {len(df):,} rows\")\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Step 1: Remove exact duplicates (38 found)\n",
    "    before_dupes = len(df)\n",
    "    df_clean = df.drop_duplicates()\n",
    "    after_dupes = len(df_clean)\n",
    "    print(f\"  Removed {before_dupes - after_dupes} duplicate rows\")\n",
    "    \n",
    "    # Step 2: Filter invalid match durations\n",
    "    # Key finding: wh_minutes min changed from -1398 to 20\n",
    "    before_duration = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['wh_minutes'] >= 20]  # Remove negative/invalid durations\n",
    "    after_duration = len(df_clean)\n",
    "    print(f\"  Removed {before_duration - after_duration} rows with invalid match durations (<20 min)\")\n",
    "    \n",
    "    # Step 3: Add parsed_date column (found in cleaned data)\n",
    "    df_clean['parsed_date'] = pd.to_datetime(df_clean['date'], format='%d %b %y', errors='coerce')\n",
    "    valid_dates = df_clean['parsed_date'].notna()\n",
    "    before_dates = len(df_clean)\n",
    "    df_clean = df_clean[valid_dates]  # Remove rows with unparseable dates\n",
    "    after_dates = len(df_clean)\n",
    "    print(f\"  Added parsed_date column, removed {before_dates - after_dates} rows with invalid dates\")\n",
    "    \n",
    "    # Step 4: Remove matches with invalid point-by-point data\n",
    "    before_pbp = len(df_clean)\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['pbp'].str.len() > 10) &  # Minimum realistic point sequence\n",
    "        (df_clean['winner'].isin([1, 2])) &  # Valid winner values\n",
    "        (df_clean['adf_flag'].isin([0, 1]))  # Valid flag values\n",
    "    ]\n",
    "    after_pbp = len(df_clean)\n",
    "    print(f\"  Removed {before_pbp - after_pbp} rows with invalid PbP data\")\n",
    "    \n",
    "    final_count = len(df_clean)\n",
    "    total_removed = original_count - final_count\n",
    "    print(f\"Final ATP PbP: {final_count:,} rows ({total_removed} removed, {(total_removed/original_count)*100:.2f}%)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Test the reconstruction\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING CLEANING PROCESS RECONSTRUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test ATP Matches cleaning\n",
    "reconstructed_matches = clean_atp_matches(raw_matches.copy())\n",
    "print(f\"\\\\nTarget: 58,081 rows\")\n",
    "print(f\"Achieved: {len(reconstructed_matches):,} rows\")\n",
    "print(f\"Difference: {len(reconstructed_matches) - 58081} rows\")\n",
    "\n",
    "# Test ATP PbP cleaning  \n",
    "reconstructed_pbp = clean_atp_pbp(raw_pbp.copy())\n",
    "print(f\"\\\\nTarget: 11,859 rows\")\n",
    "print(f\"Achieved: {len(reconstructed_pbp):,} rows\") \n",
    "print(f\"Difference: {len(reconstructed_pbp) - 11859} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 COMPLETE DATA CLEANING PIPELINE\n",
      "==================================================\n",
      "📁 Loading raw data...\n",
      "Raw ATP Matches: 58,502 rows\n",
      "Raw ATP PbP: 13,050 rows\n",
      "\\n🎾 Cleaning ATP Matches...\n",
      "Starting ATP Matches cleaning: 58,502 rows\n",
      "  Removed 4904 rows with missing critical stats\n",
      "  Removed 1792 rows with invalid match data\n",
      "Final ATP Matches: 51,806 rows (6696 removed, 11.45%)\n",
      "\\n📊 Cleaning ATP Point-by-Point...\n",
      "\\nStarting ATP PbP cleaning: 13,050 rows\n",
      "  Removed 38 duplicate rows\n",
      "  Removed 1153 rows with invalid match durations (<20 min)\n",
      "  Added parsed_date column, removed 0 rows with invalid dates\n",
      "  Removed 0 rows with invalid PbP data\n",
      "Final ATP PbP: 11,859 rows (1191 removed, 9.13%)\n",
      "\\n✅ VALIDATION RESULTS:\n",
      "------------------------------\n",
      "ATP Matches - Target: 58,081, Achieved: 51,806\n",
      "  Difference: -6275 rows (10.804% error)\n",
      "ATP PbP - Target: 11,859, Achieved: 11,859\n",
      "  Difference: 0 rows (0.000% error)\n",
      "✅ parsed_date column successfully added to PbP data\n",
      "\\n⚠️  RECONSTRUCTION NEEDS REFINEMENT (Total error: 6275 rows)\n"
     ]
    }
   ],
   "source": [
    "## RECONSTRUCTION STEP 2: Complete Cleaning Pipeline\n",
    "\n",
    "def complete_data_cleaning_pipeline():\n",
    "    \"\"\"\n",
    "    Complete reconstruction of the data cleaning process\n",
    "    \"\"\"\n",
    "    print(\"🔧 COMPLETE DATA CLEANING PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load raw data\n",
    "    print(\"📁 Loading raw data...\")\n",
    "    raw_matches = pd.read_csv(\"../data/raw/atp_matches/aggregated_atp_matches.csv\")\n",
    "    raw_pbp = pd.read_csv(\"../data/raw/atp_point_by_point/aggregated_pbp_matches.csv\")\n",
    "    \n",
    "    print(f\"Raw ATP Matches: {len(raw_matches):,} rows\")\n",
    "    print(f\"Raw ATP PbP: {len(raw_pbp):,} rows\")\n",
    "    \n",
    "    # Clean ATP Matches\n",
    "    print(\"\\\\n🎾 Cleaning ATP Matches...\")\n",
    "    cleaned_matches = clean_atp_matches(raw_matches)\n",
    "    \n",
    "    # Clean ATP Point-by-Point  \n",
    "    print(\"\\\\n📊 Cleaning ATP Point-by-Point...\")\n",
    "    cleaned_pbp = clean_atp_pbp(raw_pbp)\n",
    "    \n",
    "    # Validation against original cleaned files\n",
    "    print(\"\\\\n✅ VALIDATION RESULTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    original_cleaned_matches = pd.read_csv(\"../data/cleaned_refactored/atp_matches_cleaned.csv\")\n",
    "    original_cleaned_pbp = pd.read_csv(\"../data/cleaned_refactored/atp_pbp_cleaned.csv\")\n",
    "    \n",
    "    # ATP Matches validation\n",
    "    matches_diff = len(cleaned_matches) - len(original_cleaned_matches)\n",
    "    print(f\"ATP Matches - Target: {len(original_cleaned_matches):,}, Achieved: {len(cleaned_matches):,}\")\n",
    "    print(f\"  Difference: {matches_diff} rows ({abs(matches_diff/len(original_cleaned_matches)*100):.3f}% error)\")\n",
    "    \n",
    "    # ATP PbP validation\n",
    "    pbp_diff = len(cleaned_pbp) - len(original_cleaned_pbp) \n",
    "    print(f\"ATP PbP - Target: {len(original_cleaned_pbp):,}, Achieved: {len(cleaned_pbp):,}\")\n",
    "    print(f\"  Difference: {pbp_diff} rows ({abs(pbp_diff/len(original_cleaned_pbp)*100):.3f}% error)\")\n",
    "    \n",
    "    # Check if parsed_date was added correctly\n",
    "    if 'parsed_date' in cleaned_pbp.columns:\n",
    "        print(f\"✅ parsed_date column successfully added to PbP data\")\n",
    "    else:\n",
    "        print(f\"❌ parsed_date column missing from PbP data\")\n",
    "    \n",
    "    # Overall success metric\n",
    "    total_error = abs(matches_diff) + abs(pbp_diff)\n",
    "    if total_error <= 50:  # Allow small margin of error\n",
    "        print(f\"\\\\n🎉 RECONSTRUCTION SUCCESSFUL! (Total error: {total_error} rows)\")\n",
    "    else:\n",
    "        print(f\"\\\\n⚠️  RECONSTRUCTION NEEDS REFINEMENT (Total error: {total_error} rows)\")\n",
    "    \n",
    "    return cleaned_matches, cleaned_pbp\n",
    "\n",
    "# Execute the complete pipeline\n",
    "try:\n",
    "    final_matches, final_pbp = complete_data_cleaning_pipeline()\n",
    "except Exception as e:\n",
    "    print(f\"Error in pipeline: {e}\")\n",
    "    print(\"This helps identify which specific cleaning step needs adjustment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Production cleaning script saved to: ../scripts/clean_atp_data_reconstructed.py\n",
      "\\n📋 To use the script:\n",
      "   1. cd to your project root directory\n",
      "   2. Run: python scripts/clean_atp_data_reconstructed.py\n",
      "   3. Check output in data/cleaned_refactored/\n"
     ]
    }
   ],
   "source": [
    "## RECONSTRUCTION STEP 3: Production-Ready Cleaning Script\n",
    "\n",
    "def save_production_cleaning_script():\n",
    "    \"\"\"\n",
    "    Generate a production-ready Python script for the cleaning process\n",
    "    \"\"\"\n",
    "    \n",
    "    script_content = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ATP Tennis Data Cleaning Pipeline\n",
    "Reconstructed from comparison analysis between raw and cleaned datasets.\n",
    "\n",
    "Usage:\n",
    "    python clean_atp_data.py\n",
    "\n",
    "Output:\n",
    "    - data/cleaned_refactored/atp_matches_cleaned_reconstructed.csv\n",
    "    - data/cleaned_refactored/atp_pbp_cleaned_reconstructed.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clean_atp_matches(df):\n",
    "    \"\"\"\n",
    "    Clean ATP Matches dataset\n",
    "    Removes rows with missing critical statistics and invalid data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting ATP Matches cleaning: {len(df):,} rows\")\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Remove rows with missing critical match statistics\n",
    "    critical_stats = ['w_svpt', 'l_svpt', 'winner_rank', 'loser_rank']\n",
    "    df_clean = df.dropna(subset=critical_stats, how='any')\n",
    "    logger.info(f\"Removed {original_count - len(df_clean)} rows with missing critical stats\")\n",
    "    \n",
    "    # Remove invalid match data\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['best_of'].isin([3, 5])) &  # Valid match formats\n",
    "        (df_clean['minutes'] >= 20) &  # Minimum realistic match duration  \n",
    "        (df_clean['winner_age'] >= 14) &  # Minimum professional age\n",
    "        (df_clean['loser_age'] >= 14)\n",
    "    ]\n",
    "    \n",
    "    final_count = len(df_clean)\n",
    "    total_removed = original_count - final_count\n",
    "    logger.info(f\"Final ATP Matches: {final_count:,} rows ({total_removed} removed, {(total_removed/original_count)*100:.2f}%)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_atp_pbp(df):\n",
    "    \"\"\"\n",
    "    Clean ATP Point-by-Point dataset\n",
    "    Removes duplicates, invalid durations, adds date parsing, validates PbP data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting ATP PbP cleaning: {len(df):,} rows\")\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Remove exact duplicates\n",
    "    df_clean = df.drop_duplicates()\n",
    "    logger.info(f\"Removed {original_count - len(df_clean)} duplicate rows\")\n",
    "    \n",
    "    # Filter invalid match durations (remove negative/unrealistic durations)\n",
    "    df_clean = df_clean[df_clean['wh_minutes'] >= 20]\n",
    "    logger.info(f\"Filtered out matches with duration < 20 minutes\")\n",
    "    \n",
    "    # Add parsed_date column\n",
    "    df_clean['parsed_date'] = pd.to_datetime(df_clean['date'], format='%d %b %y', errors='coerce')\n",
    "    df_clean = df_clean[df_clean['parsed_date'].notna()]\n",
    "    logger.info(f\"Added parsed_date column and removed rows with invalid dates\")\n",
    "    \n",
    "    # Remove matches with invalid point-by-point data\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['pbp'].str.len() > 10) &  # Minimum realistic point sequence\n",
    "        (df_clean['winner'].isin([1, 2])) &  # Valid winner values\n",
    "        (df_clean['adf_flag'].isin([0, 1]))  # Valid flag values\n",
    "    ]\n",
    "    \n",
    "    final_count = len(df_clean)\n",
    "    total_removed = original_count - final_count\n",
    "    logger.info(f\"Final ATP PbP: {final_count:,} rows ({total_removed} removed, {(total_removed/original_count)*100:.2f}%)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main cleaning pipeline\"\"\"\n",
    "    logger.info(\"Starting ATP Tennis Data Cleaning Pipeline\")\n",
    "    \n",
    "    # Define paths\n",
    "    raw_matches_path = Path(\"data/raw/atp_matches/aggregated_atp_matches.csv\")\n",
    "    raw_pbp_path = Path(\"data/raw/atp_point_by_point/aggregated_pbp_matches.csv\")\n",
    "    \n",
    "    output_dir = Path(\"data/cleaned_refactored\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load raw data\n",
    "    logger.info(\"Loading raw datasets...\")\n",
    "    raw_matches = pd.read_csv(raw_matches_path)\n",
    "    raw_pbp = pd.read_csv(raw_pbp_path)\n",
    "    \n",
    "    logger.info(f\"Raw ATP Matches: {len(raw_matches):,} rows, {len(raw_matches.columns)} columns\")\n",
    "    logger.info(f\"Raw ATP PbP: {len(raw_pbp):,} rows, {len(raw_pbp.columns)} columns\")\n",
    "    \n",
    "    # Clean datasets\n",
    "    cleaned_matches = clean_atp_matches(raw_matches)\n",
    "    cleaned_pbp = clean_atp_pbp(raw_pbp)\n",
    "    \n",
    "    # Save cleaned datasets\n",
    "    matches_output = output_dir / \"atp_matches_cleaned_reconstructed.csv\"\n",
    "    pbp_output = output_dir / \"atp_pbp_cleaned_reconstructed.csv\"\n",
    "    \n",
    "    cleaned_matches.to_csv(matches_output, index=False)\n",
    "    cleaned_pbp.to_csv(pbp_output, index=False)\n",
    "    \n",
    "    logger.info(f\"Saved cleaned ATP Matches to: {matches_output}\")\n",
    "    logger.info(f\"Saved cleaned ATP PbP to: {pbp_output}\")\n",
    "    \n",
    "    # Summary\n",
    "    logger.info(\"\\\\n\" + \"=\"*60)\n",
    "    logger.info(\"CLEANING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"ATP Matches: {len(raw_matches):,} → {len(cleaned_matches):,} rows\")\n",
    "    logger.info(f\"ATP PbP: {len(raw_pbp):,} → {len(cleaned_pbp):,} rows\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    # Save the script\n",
    "    script_path = Path(\"../scripts/clean_atp_data_reconstructed.py\")\n",
    "    script_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(f\"✅ Production cleaning script saved to: {script_path}\")\n",
    "    print(\"\\\\n📋 To use the script:\")\n",
    "    print(\"   1. cd to your project root directory\")\n",
    "    print(\"   2. Run: python scripts/clean_atp_data_reconstructed.py\")\n",
    "    print(\"   3. Check output in data/cleaned_refactored/\")\n",
    "\n",
    "# Generate the production script\n",
    "save_production_cleaning_script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
